# Lightweight-Webcam-Eye-Tracking
A **webcam-only** eye-tracking system that works on **any screen size**.  
Edit this line in `main.py` to set your exact window size/flags:
```python
screen = pygame.display.set_mode((1920, 1080), pygame.NOFRAME)
```
It has been tested on large displays where traditional IR eye trackers often struggle or are impractical.
The system extracts 478Ã—3 MediaPipe FaceMesh landmarks and learns lightweight ML regressors to predict on-screen gaze coordinates in real time. Includes fast calibration, testing UI, and tracking.

---

## TL;DR
- **Task**: Map facial mesh features â†’ 2D screen coordinates (gaze point) on any display
- **Method:** Webcam + MediaPipe FaceMesh â†’ SGD / Ridge / MLP / SVR / XGBoost regressors for (x, y)
- **Motivation:** Avoid specialized hardware; robust at larger viewing distances where classic IR trackers may not function well
- **UI:** Calibration, Test (with success tally), Track

## ğŸ“Œ Overview
This project uses dense face/iris landmarks (MediaPipe FaceMesh) and direct regression to screen coordinates, paired with smooth-moving calibration thatâ€™s quick and tolerant to head motion. It supports any screenâ€”just set the resolution you want with `pygame.display.set_mode(...)`. The approach is especially practical for large displays and kiosk-like setups.

**Key Features**
- Webcam-only pipeline with MediaPipe FaceMesh (refined landmarks)
- Fast calibration modes (smooth path, edges, random)
- Lightweight ML regressors (SGD, Ridge, MLP, SVR, XGB) selectable at runtime
- Guard-box: auto-pauses when you leave the allowed head region; resumes on return
- Optional region map heatmap of collected samples

## Method
- **Landmarks & Features:** Camera frames â†’ MediaPipe FaceMesh â†’ 478 Ã— (x,y,z) flattened vector
- **Screen Prediction:** Two regressors: fâ‚“(features) â†’ x and fáµ§(features) â†’ y
- **Calibration:** Moving target / edge / random sequences; each frame logs `[features..., target_x, target_y]` to CSV
- **Testing:** Randomly placed green rectangle; move the dot inside and hit E to record success
- **Tracking:** Continuous prediction with temporal averaging for smoother motion

## ğŸ“¦ Project Structure
```graphql
eye-tracking/
â”œâ”€ main.py                 # Calibrate / Test / Track UI (pygame)
â”œâ”€ Gaze.py                 # MediaPipe FaceMesh capture + guard-box (OpenCV)
â”œâ”€ Target.py               # Target (dot) + Test rectangle rendering (pygame)
â”œâ”€ utils.py                # Config, region map, helpers
â”œâ”€ create_models.py        # (Optional) offline training example
â”œâ”€ Eye_tracking_system.pdf # Method & results paper
â”œâ”€ config.ini              # Your settings (see below)
â”œâ”€ data/                   # region_map.npy (auto)
â”œâ”€ data_csvs/              # calibration CSVs (auto)
â””â”€ test_results/           # per-model test logs (auto)
```

## âš™ï¸ Setup
### 1. Clone the repo
```bash
git clone https://github.com/Anomaly33/Lightweight-Webcam-Eye-Tracking.git
cd Lightweight-Webcam-Eye-Tracking
```
### 2. Create Environment
```bash
python -m venv .venv
source .venv/bin/activate    # Linux/Mac
# On Windows: .venv\Scripts\activate
```
### 3. Install dependencies
```bash
pip install -r requirements.txt
```
### 4. Config(`config.ini`)
